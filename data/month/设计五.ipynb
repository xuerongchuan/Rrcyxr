{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvUff_vCOzuf"
   },
   "source": [
    "## NAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yKnDDakeKqTD"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JtST4qc5hIAg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def getTime(timeVal):\n",
    "  t = datetime.datetime.strptime(timeVal, '%Y-%m-%d')\n",
    "  d = t.timetuple()\n",
    "  return int(time.mktime(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf6398iWiQZf"
   },
   "source": [
    "获得时间节点\n",
    "\n",
    "\n",
    "1.   00/4/25-00/9/1\n",
    "2.   00/9/1-01/1/1\n",
    "3.   01/1/1-01/5/1\n",
    "4.   01/5/1-01/9/1\n",
    "5.   01/9/1-02/1/1\n",
    "6.   02/1/1-02/5/1\n",
    "7.   02/5/1-02/9/1\n",
    "8.   02-9/1 - 03/1/1\n",
    "9.   03/1/1 - 03/2/28\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8UKIDBQfiAEX"
   },
   "outputs": [],
   "source": [
    "numT = 9\n",
    "c1 =  getTime('2000-9-1')\n",
    "c2 = getTime('2001-1-1')\n",
    "c3 = getTime('2001-5-1')\n",
    "c4 = getTime('2001-9-1')\n",
    "c5 = getTime('2002-1-1')\n",
    "c6 = getTime('2002-5-1')\n",
    "c7 = getTime('2002-9-1')\n",
    "c8 = getTime('2003-1-1')\n",
    "c9 = getTime('2003-3-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numT = 31\n",
    "c1 =  getTime('2000-9-1')\n",
    "c2 =  getTime('2000-10-1')\n",
    "c3=  getTime('2000-11-1')\n",
    "c4 =  getTime('2000-12-1')\n",
    "c5 = getTime('2001-1-1')\n",
    "c6 =  getTime('2001-2-1')\n",
    "c7 =  getTime('2001-3-1')\n",
    "c8 =  getTime('2001-4-1')\n",
    "c9 = getTime('2001-5-1')\n",
    "c10 = getTime('2001-6-1')\n",
    "c11 = getTime('2001-7-1')\n",
    "c12 = getTime('2001-8-1')\n",
    "c13 = getTime('2001-9-1')\n",
    "c14 = getTime('2001-10-1')\n",
    "c15 = getTime('2001-11-1')\n",
    "c16 = getTime('2001-12-1')\n",
    "c17 = getTime('2002-1-1')\n",
    "c18 =  getTime('2002-2-1')\n",
    "c19 =  getTime('2002-3-1')\n",
    "c20 =  getTime('2002-4-1')\n",
    "c21 = getTime('2002-5-1')\n",
    "c22 = getTime('2002-6-1')\n",
    "c23 = getTime('2002-7-1')\n",
    "c24 = getTime('2002-8-1')\n",
    "c25 = getTime('2002-9-1')\n",
    "c26 = getTime('2002-10-1')\n",
    "c27 = getTime('2002-11-1')\n",
    "c28 = getTime('2002-12-1')\n",
    "c29 = getTime('2003-1-1')\n",
    "c30 = getTime('2003-2-1')\n",
    "c31 = getTime('2003-3-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5f6yPAQkKurK"
   },
   "outputs": [],
   "source": [
    "def getDate(timestamp):\n",
    "  timeArray = time.localtime(timestamp)\n",
    "  timestr = time.strftime(\"%Y-%m-%d %H:%M:%S\", timeArray)\n",
    "  date = datetime.datetime.strptime(timestr, \"%Y-%m-%d %H:%M:%S\")\n",
    "  return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqfAfZdcLHR0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "93J2n60Fn30F"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oRbgyfwnOytf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lfWWGHYJO8BI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Bvuv2o_UQv2j"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Data/ml-1m.train.rating', header=None, names=['userId', 'movieId', 'rating', 'timestamp'],sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Bvuv2o_UQv2j"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('Data/ml-1m.test.rating', header=None, names=['userId', 'movieId', 'rating', 'timestamp'],sep='\\t', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Bvuv2o_UQv2j"
   },
   "outputs": [],
   "source": [
    "test_neg_dict = {}\n",
    "with open('Data/ml-1m.test.negative','r') as f:\n",
    "    for line in f.readlines():\n",
    "        test_neg_data = [int(i) for i in line.split('\\t')[1:]]\n",
    "        b = re.findall(r'[(](.*?),(.*?)[)]',  line.split('\\t')[0])\n",
    "        index_tuple = (int(b[0][0]), int(b[0][1]))\n",
    "        test_neg_dict[index_tuple] = test_neg_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-gmo8_O0CQB"
   },
   "source": [
    "划分 测试集训练集 --leave-one-out\n",
    "\n",
    "保证每个用户的最后一个rating作为测试集\n",
    "\n",
    "负样本采样，对于热门商品，为每个用户采样99个负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nylDpxGTREMg"
   },
   "outputs": [],
   "source": [
    "train_data['label'] = np.array([1]*len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_items = max(train_data.movieId.unique())+1\n",
    "num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kIO16ke74xGz"
   },
   "outputs": [],
   "source": [
    "def generate_item_time_map_dict(data):\n",
    "    \n",
    "  grouped = data.groupby(['movieId']).mean()\n",
    "  return dict(zip(grouped.index, pd.cut(grouped['timestamp'].values, [0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18, \\\n",
    "                                                                     c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31], \\\n",
    "                                        labels=list(range(31)))))\n",
    "\n",
    "ITmap = dict(generate_item_time_map_dict(train_data[['movieId','timestamp']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6Ufux4rC30ER"
   },
   "outputs": [],
   "source": [
    "train_data['timestamp']= pd.cut(train_data.timestamp, [0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18, \\\n",
    "                                                                     c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31], \\\n",
    "                                        labels=list(range(31)))\n",
    "test_data['timestamp']= pd.cut(test_data.timestamp, [0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18, \\\n",
    "                                                                     c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31], \\\n",
    "                                        labels=list(range(31)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "niOJwTG5SaeQ"
   },
   "outputs": [],
   "source": [
    "def generate_negative_samples(uindex, data, neg_count = 4):\n",
    "    history = list(data[data.userId==uindex].movieId.values)\n",
    "    negative_items = []\n",
    "    for tmp in range(neg_count):\n",
    "        j = np.random.choice(data.movieId.unique())\n",
    "        while j in history:\n",
    "            j = np.random.choice(data.movieId.unique())\n",
    "        negative_items.append((j , 0 , ITmap[j]))\n",
    "        \n",
    "    return negative_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_train_list(data):\n",
    "    train_list = []\n",
    "    for u in data.userId.unique():\n",
    "        train_list.append([(i[0], i[1])for i in data[data.userId==u][['movieId','timestamp']].values])\n",
    "    return train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_zjPxcQQ4f41"
   },
   "outputs": [],
   "source": [
    "train_list = generate_train_list(train_data)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['label'] = np.array([0]*len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = generate_train_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_items):\n",
    "    if i not in ITmap.keys():\n",
    "        ITmap[i] = numT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_all(test_list):\n",
    "    test_all = []\n",
    "    for ind, element in enumerate(test_list):\n",
    "        test_all_u = []\n",
    "        index = (ind, element[0][0])\n",
    "        neg_items = test_neg_dict[index]\n",
    "        for i in neg_items:\n",
    "            test_all_u.append((i,ITmap[i]))\n",
    "        test_all_u.append(element[0])\n",
    "        test_all.append(test_all_u)\n",
    "    return test_all\n",
    "\n",
    "test_all = generate_test_all(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_count = 4\n",
    "train_all = [ ]\n",
    "for u in range(len(train_list)):\n",
    "    u_train_all = []\n",
    "    history = list(train_data[train_data.userId==u].movieId.values)\n",
    "    for i in train_list[u]:\n",
    "        u_train_all.append(i)\n",
    "        for tmp in range(neg_count):\n",
    "            j = np.random.choice(train_data.movieId.unique())\n",
    "            while j in history:\n",
    "                j = np.random.choice(train_data.movieId.unique())\n",
    "            u_train_all.append((j , 0 , ITmap[j]))\n",
    "    train_all.append(u_train_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def default(o):\n",
    "    if isinstance(o, np.int64): return int(o)  \n",
    "    raise TypeError\n",
    "with open('test_u' ,'a') as f:\n",
    "    for line in test_all:\n",
    "        linestr = json.dumps(line, default=default)\n",
    "        f.write(linestr)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test_u' ,'a') as f:\n",
    "    for line in train_list:\n",
    "        linestr = json.dumps(line, default=default)\n",
    "        f.write(linestr)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITmap = dict([(str(x), y) for x,y in ITmap.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('itmap', 'w') as f:\n",
    "    json.dump(ITmap, f, default=default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gRepRv8fhKxW"
   },
   "outputs": [],
   "source": [
    "embedding_size =16\n",
    "weight_size = 16\n",
    "verbose = 20\n",
    "epoches = 200\n",
    "beta = 0.5\n",
    "alpha = 0\n",
    "regs = [1e-5, 1e-7, 1e-4]\n",
    "#regs = [0, 0, 0]\n",
    "lambda_bilinear = regs[0]\n",
    "gamma_bilinear = regs[1]\n",
    "eta_bilinear = regs[2]\n",
    "lr = 0.01\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "g0R8J3VjTv-u"
   },
   "outputs": [],
   "source": [
    "def generate_train_data(train_list):\n",
    "    u_batches = []\n",
    "    ut_batches = []\n",
    "    i_batches = []\n",
    "    t_batches = []\n",
    "    num_batches = []\n",
    "    label_batches = []\n",
    "    for u in range(len(train_list)):\n",
    "        u_data = []\n",
    "        u_tdata = []\n",
    "        i_data=[]\n",
    "        t_data = []\n",
    "        num_data = []\n",
    "        labels = []\n",
    "        count =0\n",
    "        for index in range(len(train_all[u])):\n",
    "            u_hdata = train_list[u].copy()\n",
    "            removed =0\n",
    "            try: \n",
    "                u_hdata.remove(u_hdata[index])\n",
    "                removed = 1\n",
    "            except Exception:\n",
    "                pass\n",
    "            items, label, times = zip(*u_hdata)\n",
    "            if removed:\n",
    "                items = list(items)\n",
    "                times = list(times)\n",
    "                items.append(num_items)\n",
    "                times.append(numT)\n",
    "            num = len(train_list[u])\n",
    "            u_data.append(items)\n",
    "            u_tdata.append(times)\n",
    "            i_data.append(train_all[u][index][0])\n",
    "            t_data.append(train_all[u][index][2])\n",
    "            labels.append(train_all[u][index][1])\n",
    "            num_data.append(num)\n",
    "        u_batches.append(np.array(u_data))\n",
    "        ut_batches.append(np.array(u_tdata))\n",
    "        i_batches.append(i_data)\n",
    "        t_batches.append(t_data)\n",
    "        num_batches.append(num_data)\n",
    "        label_batches.append(labels)\n",
    "\n",
    "    return u_batches, ut_batches, i_batches, t_batches, label_batches, num_batches\n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xw0RwAvp0c4W"
   },
   "outputs": [],
   "source": [
    "u_batches, ut_batches, i_batches, t_batches, label_batches, num_batches = generate_train_data(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_test_data(test_list):\n",
    "    u_batches = []\n",
    "    ut_batches = []\n",
    "    i_batches = []\n",
    "    t_batches = []\n",
    "    num_batches = []\n",
    "    label_batches = []\n",
    "    for u in range(len(test_list)):\n",
    "        u_data = []\n",
    "        u_tdata = []\n",
    "        i_data=[]\n",
    "        t_data = []\n",
    "        num_data = []\n",
    "        labels = []\n",
    "        for index in range(len(test_all[u])):\n",
    "            u_hdata = train_list[u].copy()\n",
    "            items, label, times = zip(*u_hdata)\n",
    "            num = len(train_list[u])\n",
    "            u_data.append(items)\n",
    "            u_tdata.append(times)\n",
    "            i_data.append(test_all[u][index][0])\n",
    "            t_data.append(test_all[u][index][2])\n",
    "            labels.append(test_all[u][index][1])\n",
    "            num_data.append(num)\n",
    "        u_batches.append(np.array(u_data))\n",
    "        ut_batches.append(np.array(u_tdata))\n",
    "        i_batches.append(i_data)\n",
    "        t_batches.append(t_data)\n",
    "        num_batches.append(num_data)\n",
    "        label_batches.append(labels)\n",
    "    return u_batches, ut_batches, i_batches, t_batches, label_batches, num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AWwT0rlrbrI-"
   },
   "outputs": [],
   "source": [
    "u_test_batches, ut_test_batches, i_test_batches, t_test_batches, label_test_batches, num_test_batches = \\\n",
    "generate_test_data(test_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFxeY6pHhqUM"
   },
   "source": [
    "### 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EazwFaFOhwJk"
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    with tf.name_scope('input_data'):\n",
    "        user_input  = tf.placeholder(tf.int32, shape = [None, None], name='user_input')\n",
    "        num_idx = tf.placeholder(tf.float32, shape=[None,1], name='num_idx')\n",
    "        item_input = tf.placeholder(tf.int32, shape=[None,1], name='item_input')\n",
    "        time_input = tf.placeholder(tf.int32, shape=[None, None], name='time_input')\n",
    "        otime_input = tf.placeholder(tf.int32, shape=[None, 1], name='otime_input')\n",
    "        labels = tf.placeholder(tf.float32, shape=[None,1], name='labels')\n",
    "        return user_input, num_idx, item_input, time_input, otime_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RvljoxoFh20c",
    "outputId": "b4b63303-3c84-4a0f-b62d-4ea416cc33ef"
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope('embeddings'):\n",
    "    c1 = tf.Variable(tf.truncated_normal(shape=[num_items, embedding_size], mean=0.0, stddev=0.01),\\\n",
    "                    name='c1', dtype = tf.float32)\n",
    "    c2 = tf.constant(0.0, tf.float32, [1, embedding_size], name='c2')\n",
    "    embedding_Q_ = tf.concat([c1, c2], 0 , name='emebedding_Q_') \n",
    "    embedding_Q = tf.Variable(tf.truncated_normal(shape=[num_items,embedding_size]), name='embediing_Q', \\\n",
    "                             dtype=tf.float32)\n",
    "    t1 = tf.Variable(tf.truncated_normal(shape=[numT+1, embedding_size], mean=0.0, stddev=0.01),\\\n",
    "                    name='t1', dtype = tf.float32)\n",
    "    t2 = tf.constant(0.0, tf.float32, [1, embedding_size], name='t2')\n",
    "    embedding_T = tf.concat([t1, t2], 0 , name='emebedding_T')\n",
    "    bias = tf.Variable(tf.zeros(num_items), name='bias')\n",
    "    \n",
    "    #attention network variables\n",
    "    W = tf.Variable(tf.truncated_normal(shape=[2*embedding_size, weight_size], mean=0.0, \\\n",
    "                    stddev=tf.sqrt(tf.div(2.0, weight_size + embedding_size))),name='Weights_for_MLP', dtype=tf.float32, trainable=True)\n",
    "    bias_b = tf.Variable(tf.truncated_normal(shape=[1, weight_size], mean=0.0, \\\n",
    "        stddev=tf.sqrt(tf.divide(2.0, weight_size + embedding_size))),name='Bias_for_MLP', dtype=tf.float32, trainable=True)\n",
    "    h = tf.Variable(tf.ones([weight_size, 1]), name='H_for_MLP', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3jeMbT2DmasY"
   },
   "outputs": [],
   "source": [
    "user_input, num_idx, item_input, time_input, otime_input, labels = get_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0pOhpcMimm4e"
   },
   "outputs": [],
   "source": [
    "def attention_MLP(q_, num_idx, B):#q_:(b,n,2e)\n",
    "    with tf.name_scope(\"attention_MLP\"):\n",
    "        b = tf.shape(q_)[0]\n",
    "        n = tf.shape(q_)[1]\n",
    "        r = 2*embedding_size\n",
    "        MLP_output = tf.matmul(tf.reshape(q_,[-1,r]), W) + bias_b #(b*n, e or 2*e) * (e or 2*e, w) + (1, w)\n",
    "        MLP_output = tf.nn.dropout(MLP_output, 0.5)#(b*n, w)\n",
    "#         fc_mean, fc_var = tf.nn.moments(MLP_output, axes=[0,1])\n",
    "#         scale = tf.Variable(tf.ones([r]))\n",
    "#         shift = tf.Variable(tf.zeros([r]))\n",
    "#         epsilon = 0.001\n",
    "#         MLP_output = tf.nn.batch_normalization(MLP_output, fc_mean, fc_var, \\\n",
    "#                                                shift, scale, epsilon)\n",
    "        \n",
    "        MLP_output = tf.nn.relu( MLP_output )\n",
    "        #添加一个dropout\n",
    "        A_ = tf.reshape(tf.matmul(MLP_output, h),[b,n]) #(b*n, w) * (w, 1) => (None, 1) => (b, n)\n",
    "        # softmax for not mask features\n",
    "        exp_A_ = tf.exp(A_)\n",
    "        mask_mat = tf.sequence_mask(tf.reduce_sum(num_idx,1), maxlen = n, dtype = tf.float32) # (b, n)\n",
    "        exp_A_ = mask_mat * exp_A_\n",
    "        exp_sum = tf.reduce_sum(exp_A_,1, keepdims=True)  # (b, 1)\n",
    "        exp_sum = tf.pow(exp_sum, tf.constant(beta, tf.float32, [1]))\n",
    "\n",
    "        A = tf.expand_dims(tf.div(exp_A_, exp_sum),2) # (b, n, 1)\n",
    "#         time_input = tf.expand_dims(time_input, 1)\n",
    "#         time_input = tf.tile(time_input, [1, n, 1])\n",
    "#         A = A + time_input\n",
    "\n",
    "        #加入时间衰减\n",
    "#         B = tf.expand_dims(B, 2)\n",
    "#         A = A*B\n",
    "      \n",
    "        return tf.reduce_sum(A * embedding_q_, 1)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jy25aGaQmp9H"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inference\"):\n",
    "    embedding_q_ = tf.nn.embedding_lookup(embedding_Q_, user_input) # (b, n, e)\n",
    "    embedding_t_ = tf.nn.embedding_lookup(embedding_T, time_input)   #(b,n,e)\n",
    "    embedding_q_ = tf.concat([embedding_q_, embedding_t_], axis=2) #(b,n,2e)\n",
    "    \n",
    "    \n",
    "    embedding_q = tf.nn.embedding_lookup(embedding_Q, item_input) # (b, 1, e)\n",
    "    embedding_t = tf.nn.embedding_lookup(embedding_T, otime_input) # (b, 1, e)\n",
    "    embedding_q = tf.concat([embedding_q, embedding_t], axis=2)#(b,1,2e)\n",
    "    embedding_p = attention_MLP(embedding_q_ * embedding_q, num_idx, time_input) #(b,2e)\n",
    "    embedding_q = tf.reduce_sum(embedding_q, 1) #(b,2e)\n",
    "    bias_i = tf.nn.embedding_lookup(bias, item_input)\n",
    "#     coeff = tf.pow(num_idx, tf.constant(alpha, tf.float32, [1]))\n",
    "#     output = tf.expand_dims(tf.reduce_sum(embedding_p*embedding_q, 1),1) + bias_i\n",
    "#     output = tf.layers.dense(embedding_p*embedding_q, units=1, activation = None)\n",
    "#     output = tf.map_fn(lambda x:x*5.0 , output)\n",
    "#     output = tf.layers.dense(embedding_p*embedding_q, units=16, activation = tf.nn.sigmoid)\n",
    "#     output = tf.layers.dense(output, 1, None)\n",
    "    output = tf.sigmoid(tf.expand_dims(tf.reduce_sum(embedding_p*embedding_q, 1),1) + bias_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Dbn6DbY4mru3",
    "outputId": "4eb17893-f5c6-4a95-952e-ef8d7e50a29a"
   },
   "outputs": [],
   "source": [
    "regs = [1e-5, 1e-7, 1e-4]\n",
    "lr = 0.01\n",
    "lambda_bilinear = regs[0]\n",
    "gamma_bilinear = regs[1]\n",
    "eta_bilinear = regs[2]\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.losses.log_loss(labels, output) + \\\n",
    "                lambda_bilinear * tf.reduce_sum(tf.abs(embedding_Q)) + \\\n",
    "                gamma_bilinear * tf.reduce_sum(tf.abs(embedding_Q_)) + \\\n",
    "                eta_bilinear * tf.reduce_sum(tf.abs(W)))\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=lr, initial_accumulator_value=1e-8).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MP2AgpPAZWb1"
   },
   "outputs": [],
   "source": [
    "epoches =200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4365
    },
    "colab_type": "code",
    "id": "dcwfZMMOnpI_",
    "outputId": "b6cc27c8-0055-4c50-dab5-34bf164adfd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [ 3.0s]: HR = 0.3000, NDCG = 0.1092, loss = 0.9850 [0.2s] train_loss = 0.9998 [0.6s]\n",
      "Epoch 20 [ 2.3s]: HR = 0.3000, NDCG = 0.1869, loss = 0.6103 [0.1s] train_loss = 0.6957 [0.5s]\n",
      "Epoch 40 [ 2.4s]: HR = 0.2800, NDCG = 0.1873, loss = 0.5844 [0.1s] train_loss = 0.6242 [0.6s]\n",
      "Epoch 60 [ 2.2s]: HR = 0.3200, NDCG = 0.1926, loss = 0.5833 [0.2s] train_loss = 0.5724 [0.6s]\n",
      "Epoch 80 [ 2.3s]: HR = 0.4000, NDCG = 0.2174, loss = 0.5891 [0.1s] train_loss = 0.5318 [0.6s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[2985,597,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node optimizer/gradients/inference/attention_MLP/mul_1_grad/Mul}} = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](optimizer/gradients/inference/attention_MLP/Sum_2_grad/Tile, inference/concat)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node optimizer/gradients/embeddings/emebedding_Q__grad/GatherV2_2/_99}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_629_optimizer/gradients/embeddings/emebedding_Q__grad/GatherV2_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'optimizer/gradients/inference/attention_MLP/mul_1_grad/Mul', defined at:\n  File \"c:\\users\\yxr\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1080, in __init__\n    self.run()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-34-80a07aff7fbf>\", line 12, in <module>\n    optimizer = tf.train.AdagradOptimizer(learning_rate=lr, initial_accumulator_value=1e-8).minimize(loss)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 401, in minimize\n    grad_loss=grad_loss)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 517, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 596, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 776, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 398, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 776, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 910, in _MulGrad\n    math_ops.reduce_sum(gen_math_ops.mul(grad, y), rx), sx),\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 5358, in mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\n...which was originally created as op 'inference/attention_MLP/mul_1', defined at:\n  File \"c:\\users\\yxr\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 29 identical lines from previous traceback]\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-58e28eac6b44>\", line 10, in <module>\n    embedding_p = attention_MLP(embedding_q_ * embedding_q, num_idx, time_input) #(b,2e)\n  File \"<ipython-input-32-afde1feebd99>\", line 34, in attention_MLP\n    return tf.reduce_sum(A * embedding_q_, 1)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 862, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1129, in _mul_dispatch\n    return gen_math_ops.mul(x, y, name=name)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 5358, in mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[2985,597,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node optimizer/gradients/inference/attention_MLP/mul_1_grad/Mul}} = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](optimizer/gradients/inference/attention_MLP/Sum_2_grad/Tile, inference/concat)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node optimizer/gradients/embeddings/emebedding_Q__grad/GatherV2_2/_99}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_629_optimizer/gradients/embeddings/emebedding_Q__grad/GatherV2_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[2985,597,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node optimizer/gradients/inference/attention_MLP/mul_1_grad/Mul}} = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](optimizer/gradients/inference/attention_MLP/Sum_2_grad/Tile, inference/concat)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node optimizer/gradients/embeddings/emebedding_Q__grad/GatherV2_2/_99}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_629_optimizer/gradients/embeddings/emebedding_Q__grad/GatherV2_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-fa877f415ae2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             feed_dict = {user_input: user_input_data, num_idx: num_idx_data[:, np.newaxis], item_input: item_input_data[:, np.newaxis],time_input:time_input_data,\n\u001b[0;32m     16\u001b[0m                         otime_input:otime_input_data[:,np.newaxis], labels: labels_data[:, np.newaxis]}\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtraining_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mtrain_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrain_begin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch_count\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[0;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[2985,597,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node optimizer/gradients/inference/attention_MLP/mul_1_grad/Mul}} = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](optimizer/gradients/inference/attention_MLP/Sum_2_grad/Tile, inference/concat)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node optimizer/gradients/embeddings/emebedding_Q__grad/GatherV2_2/_99}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_629_optimizer/gradients/embeddings/emebedding_Q__grad/GatherV2_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'optimizer/gradients/inference/attention_MLP/mul_1_grad/Mul', defined at:\n  File \"c:\\users\\yxr\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1080, in __init__\n    self.run()\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-34-80a07aff7fbf>\", line 12, in <module>\n    optimizer = tf.train.AdagradOptimizer(learning_rate=lr, initial_accumulator_value=1e-8).minimize(loss)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 401, in minimize\n    grad_loss=grad_loss)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 517, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 596, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 776, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 398, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 776, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 910, in _MulGrad\n    math_ops.reduce_sum(gen_math_ops.mul(grad, y), rx), sx),\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 5358, in mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\n...which was originally created as op 'inference/attention_MLP/mul_1', defined at:\n  File \"c:\\users\\yxr\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 29 identical lines from previous traceback]\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-58e28eac6b44>\", line 10, in <module>\n    embedding_p = attention_MLP(embedding_q_ * embedding_q, num_idx, time_input) #(b,2e)\n  File \"<ipython-input-32-afde1feebd99>\", line 34, in attention_MLP\n    return tf.reduce_sum(A * embedding_q_, 1)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 862, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1129, in _mul_dispatch\n    return gen_math_ops.mul(x, y, name=name)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 5358, in mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"c:\\users\\yxr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[2985,597,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node optimizer/gradients/inference/attention_MLP/mul_1_grad/Mul}} = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](optimizer/gradients/inference/attention_MLP/Sum_2_grad/Tile, inference/concat)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node optimizer/gradients/embeddings/emebedding_Q__grad/GatherV2_2/_99}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_629_optimizer/gradients/embeddings/emebedding_Q__grad/GatherV2_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_batch = len(u_batches)\n",
    "    batch_index = range(num_batch)\n",
    "    for epoch_count in range(epoches):\n",
    "        #train\n",
    "        train_begin = time.time()\n",
    "        for index in batch_index:\n",
    "            user_input_data = u_batches[index].astype(np.int32)\n",
    "            num_idx_data= np.array(num_batches[index]).astype(np.float32)\n",
    "            item_input_data= np.array(i_batches[index]).astype(np.int32)\n",
    "            time_input_data = ut_batches[index].astype(np.float32)\n",
    "            otime_input_data = np.array(t_batches[index])\n",
    "            labels_data = np.array(label_batches[index]).astype(np.float32)\n",
    "            feed_dict = {user_input: user_input_data, num_idx: num_idx_data[:, np.newaxis], item_input: item_input_data[:, np.newaxis],time_input:time_input_data,\n",
    "                        otime_input:otime_input_data[:,np.newaxis], labels: labels_data[:, np.newaxis]}\n",
    "            training_loss, _ = sess.run([loss, optimizer], feed_dict)\n",
    "        train_time = time.time() - train_begin\n",
    "        if epoch_count % verbose == 0:\n",
    "            #train loss\n",
    "            loss_begin = time.time()\n",
    "            train_loss = 0.0\n",
    "            for index in range(num_batch):\n",
    "                user_input_data = u_batches[index].astype(np.int32)\n",
    "                \n",
    "                num_idx_data= np.array(num_batches[index]).astype(np.float32)\n",
    "                item_input_data= np.array(i_batches[index]).astype(np.int32)\n",
    "                time_input_data = ut_batches[index].astype(np.float32)\n",
    "                otime_input_data = np.array(t_batches[index])\n",
    "                \n",
    "                labels_data = np.array(label_batches[index]).astype(np.float32)\n",
    "                feed_dict = {user_input: user_input_data, num_idx: num_idx_data[:, np.newaxis], item_input: item_input_data[:, np.newaxis],time_input:time_input_data,\n",
    "                            otime_input:otime_input_data[:,np.newaxis],labels: labels_data[:, np.newaxis]}\n",
    "                train_loss += sess.run(loss, feed_dict)\n",
    "            train_loss = train_loss/num_batch\n",
    "            loss_time = time.time() - loss_begin\n",
    "            #evaluate\n",
    "            eval_begin = time.time() \n",
    "            hits, ndcgs, losses = [],[],[]\n",
    "            for index in range(num_batch):\n",
    "                user_input_data = u_test_batches[index].astype(np.int32)\n",
    "                num_idx_data= np.array(num_test_batches[index]).astype(np.float32)\n",
    "                item_input_data= np.array(i_test_batches[index]).astype(np.int32)\n",
    "                time_input_data = ut_test_batches[index].astype(np.float32)\n",
    "                otime_input_data = np.array(t_test_batches[index])\n",
    "                \n",
    "                labels_data = np.array(label_test_batches[index]).astype(np.float32)\n",
    "                feed_dict = {user_input: user_input_data, num_idx: num_idx_data[:, np.newaxis], item_input: item_input_data[:, np.newaxis],time_input:time_input_data,\n",
    "                        otime_input:otime_input_data[:,np.newaxis],labels: labels_data[:, np.newaxis]}\n",
    "                predictions,test_loss = sess.run([output,loss], feed_dict = feed_dict)\n",
    "                predictions = predictions.flatten()\n",
    "#                 min_ = np.min(predictions)\n",
    "#                 max_ = np.max(predictions)\n",
    "#                 print(min_, max_)\n",
    "#                 predictions = np.apply_along_axis(lambda x: 5*(x-min_)/(max_-min_), 0,predictions)\n",
    "\n",
    "                neg_predict, pos_predict = predictions[:-1], predictions[-1]\n",
    "                position = (neg_predict >= pos_predict).sum()\n",
    "                hr = position < 10\n",
    "                ndcg = math.log(2) / math.log(position+2) if hr else 0\n",
    "                hits.append(hr)\n",
    "                ndcgs.append(ndcg)  \n",
    "                losses.append(test_loss)\n",
    "            hr, ndcg, test_loss = np.array(hits).mean(), np.array(ndcgs).mean(), np.array(losses).mean()\n",
    "            eval_time = time.time() - eval_begin\n",
    "#             print(\"Epoch %d [%.1fs ]:  train_loss = %.4f\" % (\n",
    "#                     epoch_count,train_time, train_loss))    \n",
    "            print(\"Epoch %d [ %.1fs]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1fs] train_loss = %.4f [%.1fs]\" % (\n",
    "                        epoch_count, train_time, hr, ndcg, test_loss, eval_time, train_loss, loss_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wYb-CejAMhQA"
   },
   "outputs": [],
   "source": [
    "beta = 6\n",
    "Epoch 0 [ 25.6s]: mae_mean = 1.0032, rmse_mean = 1.2028, loss = 1.8611 [3.1s] train_loss = 1.8357 [9.9s]\n",
    "Epoch 20 [ 24.9s]: mae_mean = 0.8833, rmse_mean = 1.0809, loss = 1.4967 [3.0s] train_loss = 1.2582 [9.9s]\n",
    "Epoch 40 [ 25.2s]: mae_mean = 0.8957, rmse_mean = 1.1006, loss = 1.5191 [3.0s] train_loss = 1.0992 [9.7s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8BYqX2LuAVFP"
   },
   "outputs": [],
   "source": [
    "Epoch 0 [ 25.9s]: mae_mean = 0.9289, rmse_mean = 1.1331, loss = 1.7034 [3.2s] train_loss = 1.6387 [10.3s]\n",
    "Epoch 20 [ 25.5s]: mae_mean = 0.8303, rmse_mean = 1.0236, loss = 1.4031 [3.1s] train_loss = 1.1656 [10.1s]\n",
    "Epoch 40 [ 26.5s]: mae_mean = 0.8180, rmse_mean = 1.0113, loss = 1.3615 [3.3s] train_loss = 1.0192 [10.7s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Lp1hsX0gD_AJ"
   },
   "outputs": [],
   "source": [
    "Epoch 0 [ 337.0s]: mae_mean = 0.9067, rmse_mean = 1.0797, loss = 1.5053 [36.5s] train_loss = 1.4938 [122.9s]\n",
    "Epoch 20 [ 346.9s]: mae_mean = 0.8075, rmse_mean = 0.9704, loss = 1.2139 [38.8s] train_loss = 1.1453 [124.9s]\n",
    "Epoch 40 [ 350.4s]: mae_mean = 0.8026, rmse_mean = 0.9657, loss = 1.1905 [36.7s] train_loss = 1.1035 [124.4s]\n",
    "Epoch 60 [ 345.5s]: mae_mean = 0.8003, rmse_mean = 0.9639, loss = 1.1794 [36.8s] train_loss = 1.0815 [123.2s]\n",
    "Epoch 80 [ 330.6s]: mae_mean = 0.7991, rmse_mean = 0.9636, loss = 1.1725 [37.6s] train_loss = 1.0639 [117.6s]\n",
    "Epoch 100 [ 338.7s]: mae_mean = 0.7996, rmse_mean = 0.9639, loss = 1.1678 [38.0s] train_loss = 1.0501 [118.2s]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epoch 0 [ 2.8s]: HR = 0.2000, NDCG = 0.0783, loss = 0.8659 [0.2s] train_loss = 0.9523 [0.7s]\n",
    "Epoch 20 [ 2.3s]: HR = 0.3400, NDCG = 0.1803, loss = 0.6116 [0.2s] train_loss = 0.6885 [0.6s]\n",
    "Epoch 40 [ 2.2s]: HR = 0.3600, NDCG = 0.1729, loss = 0.5844 [0.2s] train_loss = 0.6090 [0.6s]\n",
    "Epoch 60 [ 2.3s]: HR = 0.4000, NDCG = 0.2148, loss = 0.5833 [0.2s] train_loss = 0.5588 [0.6s]\n",
    "Epoch 80 [ 2.2s]: HR = 0.4000, NDCG = 0.1962, loss = 0.5807 [0.2s] train_loss = 0.5181 [0.6s]\n",
    "Epoch 100 [ 2.3s]: HR = 0.3800, NDCG = 0.1984, loss = 0.5866 [0.2s] train_loss = 0.4882 [0.6s]\n",
    "Epoch 120 [ 2.4s]: HR = 0.3800, NDCG = 0.2077, loss = 0.5917 [0.2s] train_loss = 0.4627 [0.6s]\n",
    "Epoch 140 [ 2.2s]: HR = 0.4000, NDCG = 0.2340, loss = 0.5920 [0.2s] train_loss = 0.4422 [0.6s]\n",
    "Epoch 160 [ 2.2s]: HR = 0.3800, NDCG = 0.1977, loss = 0.5980 [0.2s] train_loss = 0.4218 [0.6s]\n",
    "Epoch 180 [ 2.3s]: HR = 0.3800, NDCG = 0.2356, loss = 0.6121 [0.2s] train_loss = 0.4090 [0.6s]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "设计五",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
